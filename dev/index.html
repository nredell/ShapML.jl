<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction · ShapML</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img src="assets/logo.png" alt="ShapML logo"/></a><div class="docs-package-name"><span class="docs-autofit">ShapML</span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Introduction</a><ul class="internal"><li><a class="tocitem" href="#Install-1"><span>Install</span></a></li><li><a class="tocitem" href="#Documentation-and-Vignettes-1"><span>Documentation and Vignettes</span></a></li><li><a class="tocitem" href="#Examples-1"><span>Examples</span></a></li></ul></li><li><span class="tocitem">Vignettes</span><ul><li><a class="tocitem" href="vignettes/consistency/">Stochastic vs. TreeSHAP</a></li></ul></li><li><a class="tocitem" href="functions/functions/">Functions</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Introduction</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Introduction</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/nredell/ShapML.jl/blob/master/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="ShapML.jl-1"><a class="docs-heading-anchor" href="#ShapML.jl-1">ShapML.jl</a><a class="docs-heading-anchor-permalink" href="#ShapML.jl-1" title="Permalink"></a></h1><p>The purpose of <code>ShapML</code> is to compute stochastic feature-level Shapley values which can be used to (a) interpret and/or (b) assess the fairness of any machine learning model. <strong><a href="https://christophm.github.io/interpretable-ml-book/shapley.html">Shapley values</a></strong> are an intuitive and theoretically sound model-agnostic diagnostic tool to understand both <strong>global feature importance</strong> across all instances in a data set and instance/row-level <strong>local feature importance</strong> in black-box machine learning models.</p><p>This package implements the algorithm described in <a href="https://link.springer.com/article/10.1007%2Fs10115-013-0679-x">Štrumbelj and Kononenko&#39;s (2014) sampling-based Shapley approximation algorithm</a> to compute the stochastic Shapley values for a given instance and model feature.</p><ul><li><p><strong>Flexibility</strong>:</p><ul><li>Shapley values can be estimated for any machine learning model using a simple user-defined <code>predict()</code> wrapper function.</li></ul></li><li><p><strong>Speed</strong>:</p><ul><li>The speed advantage of <code>ShapML</code> comes in the form of giving the user the ability to select 1 or more target features of interest and avoid having to compute Shapley values for all model features (i.e., a subset of target features from a trained model will return the same feature-level Shapley values as the full model with all features). This is especially useful in high-dimensional models as the computation of a Shapley value is exponential in the number of features.</li></ul></li></ul><h2 id="Install-1"><a class="docs-heading-anchor" href="#Install-1">Install</a><a class="docs-heading-anchor-permalink" href="#Install-1" title="Permalink"></a></h2><pre><code class="language-julia">using Pkg
Pkg.add(&quot;ShapML&quot;)</code></pre><h2 id="Documentation-and-Vignettes-1"><a class="docs-heading-anchor" href="#Documentation-and-Vignettes-1">Documentation and Vignettes</a><a class="docs-heading-anchor-permalink" href="#Documentation-and-Vignettes-1" title="Permalink"></a></h2><ul><li><p><strong><a href="https://nredell.github.io/ShapML.jl/dev/">Docs</a></strong></p></li><li><p><strong><a href="https://nredell.github.io/ShapML.jl/dev/vignettes/consistency/">Consistency with TreeSHAP</a></strong></p></li><li><p><strong><a href="https://nredell.github.io/docs/julia_speed">Speed - Julia vs Python vs R</a></strong></p></li></ul><h2 id="Examples-1"><a class="docs-heading-anchor" href="#Examples-1">Examples</a><a class="docs-heading-anchor-permalink" href="#Examples-1" title="Permalink"></a></h2><h3 id="Random-Forest-regression-model-Non-parallel-1"><a class="docs-heading-anchor" href="#Random-Forest-regression-model-Non-parallel-1">Random Forest regression model - Non-parallel</a><a class="docs-heading-anchor-permalink" href="#Random-Forest-regression-model-Non-parallel-1" title="Permalink"></a></h3><ul><li><p>We&#39;ll explain the impact of 13 features from the Boston Housing dataset on the predicted outcome <code>MedV</code>–or the median value of owner-occupied homes in 1000&#39;s–using predictions from a trained Random Forest regression model and stochastic Shapley values.</p></li><li><p>We&#39;ll explain a subset of 300 instances and then assess global feature importance by aggregating the unique feature importances for each of these instances.</p></li></ul><pre><code class="language-julia">using ShapML
using RDatasets
using DataFrames
using MLJ  # Machine learning
using Gadfly  # Plotting

# Load data.
boston = RDatasets.dataset(&quot;MASS&quot;, &quot;Boston&quot;)
#------------------------------------------------------------------------------
# Train a machine learning model; currently limited to single outcome regression and binary classification.
outcome_name = &quot;MedV&quot;

# Data prep.
y, X = MLJ.unpack(boston, ==(Symbol(outcome_name)), colname -&gt; true)

# Instantiate an ML model; choose any single-outcome ML model from any package.
random_forest = @load RandomForestRegressor pkg = &quot;DecisionTree&quot;
model = MLJ.machine(random_forest, X, y)

# Train the model.
fit!(model)

# Create a wrapper function that takes the following positional arguments: (1) a
# trained ML model from any Julia package, (2) a DataFrame of model features. The
# function should return a 1-column DataFrame of predictions--column names do not matter.
function predict_function(model, data)
  data_pred = DataFrame(y_pred = predict(model, data))
  return data_pred
end
#------------------------------------------------------------------------------
# ShapML setup.
explain = copy(boston[1:300, :]) # Compute Shapley feature-level predictions for 300 instances.
explain = select(explain, Not(Symbol(outcome_name)))  # Remove the outcome column.

reference = copy(boston)  # An optional reference population to compute the baseline prediction.
reference = select(reference, Not(Symbol(outcome_name)))

sample_size = 60  # Number of Monte Carlo samples.
#------------------------------------------------------------------------------
# Compute stochastic Shapley values.
data_shap = ShapML.shap(explain = explain,
                        reference = reference,
                        model = model,
                        predict_function = predict_function,
                        sample_size = sample_size,
                        seed = 1
                        )

show(data_shap, allcols = true)</code></pre><p><img src="shapoutput.PNG" alt="shapoutput"/></p><ul><li><p>Now we&#39;ll create several plots that summarize the Shapley results for our Random Forest model. These plots will eventually be refined and incorporated into <code>ShapML</code>.</p></li><li><p><strong>Global feature importance</strong></p><ul><li>Because Shapley values represent deviations from the average or baseline prediction, plotting their average absolute value for each feature gives a sense of the magnitude with which they affect model predictions across all explained instances.</li></ul></li></ul><pre><code class="language-julia">data_plot = DataFrames.by(data_shap, [:feature_name],
                          mean_effect = [:shap_effect] =&gt; x -&gt; mean(abs.(x.shap_effect)))

data_plot = sort(data_plot, order(:mean_effect, rev = true))

baseline = round(data_shap.intercept[1], digits = 1)

p = plot(data_plot, y = :feature_name, x = :mean_effect, Coord.cartesian(yflip = true),
         Scale.y_discrete, Geom.bar(position = :dodge, orientation = :horizontal),
         Theme(bar_spacing = 1mm),
         Guide.xlabel(&quot;|Shapley effect| (baseline = $baseline)&quot;), Guide.ylabel(nothing),
         Guide.title(&quot;Feature Importance - Mean Absolute Shapley Value&quot;))
p</code></pre><p><img src="featureimportance.png" alt="featureimportance"/></p><ul><li><strong>Global feature effects</strong><ul><li>The plot below shows how changing the value of the <code>Rm</code> feature–the most influential feature overall–affects model predictions (holding the other features constant). Each point represents 1 of our 300 explained instances. The black line is a loess line of best fit to summarize the effect.</li></ul></li></ul><pre><code class="language-julia">data_plot = data_shap[data_shap.feature_name .== &quot;Rm&quot;, :]  # Selecting 1 feature for ease of plotting.

baseline = round(data_shap.intercept[1], digits = 1)

p_points = layer(data_plot, x = :feature_value, y = :shap_effect, Geom.point())
p_line = layer(data_plot, x = :feature_value, y = :shap_effect, Geom.smooth(method = :loess, smoothing = 0.5),
               style(line_width = 0.75mm,), Theme(default_color = &quot;black&quot;))
p = plot(p_line, p_points, Guide.xlabel(&quot;Feature value&quot;), Guide.ylabel(&quot;Shapley effect (baseline = $baseline)&quot;),
         Guide.title(&quot;Feature Effect - $(data_plot.feature_name[1])&quot;))
p</code></pre><p><img src="featureeffects.png" alt="featureeffects"/></p><hr/><h3 id="Random-Forest-regression-model-Parallel-1"><a class="docs-heading-anchor" href="#Random-Forest-regression-model-Parallel-1">Random Forest regression model - Parallel</a><a class="docs-heading-anchor-permalink" href="#Random-Forest-regression-model-Parallel-1" title="Permalink"></a></h3><ul><li><p>We&#39;ll explain the same dataset with the same model, but this time we&#39;ll compute the Shapley values in parallel across cores using the built-in distributed computing in <code>ShapML</code> which implements <code>Distributed.pmap()</code> internally.</p></li><li><p>The stochastic Shapley values will be computed in parallel over 6 cores on the same machine.</p></li><li><p>With the same seed set, <strong>non-parallel and parallel computation will return the same results</strong>.</p></li></ul><pre><code class="language-julia">using Distributed
addprocs(6)  # 6 cores.</code></pre><ul><li>The <code>@everywhere</code> block of code will load the relevant packages on each core. If you use another ML package, you would swap it in for <code>using MLJ</code>.</li></ul><pre><code class="language-julia">@everywhere begin
  using ShapML
  using DataFrames
  using MLJ
end</code></pre><pre><code class="language-julia">using RDatasets

# Load data.
boston = RDatasets.dataset(&quot;MASS&quot;, &quot;Boston&quot;)
#------------------------------------------------------------------------------
# Train a machine learning model; currently limited to single outcome regression and binary classification.
outcome_name = &quot;MedV&quot;

# Data prep.
y, X = MLJ.unpack(boston, ==(Symbol(outcome_name)), colname -&gt; true)

# Instantiate an ML model; choose any single-outcome ML model from any package.
random_forest = @load RandomForestRegressor pkg = &quot;DecisionTree&quot;
model = MLJ.machine(random_forest, X, y)

# Train the model.
fit!(model)</code></pre><ul><li><code>@everywhere</code> is needed to properly initialize the <code>predict()</code> wrapper function.</li></ul><pre><code class="language-julia"># Create a wrapper function that takes the following positional arguments: (1) a
# trained ML model from any Julia package, (2) a DataFrame of model features. The
# function should return a 1-column DataFrame of predictions--column names do not matter.
@everywhere function predict_function(model, data)
  data_pred = DataFrame(y_pred = predict(model, data))
  return data_pred
end</code></pre><ul><li>Notice that we&#39;ve set <code>ShapML.shap(parallel = :samples)</code> to perform the computation in parallel across our 60 Monte Carlo samples.</li></ul><pre><code class="language-julia"># ShapML setup.
explain = copy(boston[1:300, :]) # Compute Shapley feature-level predictions for 300 instances.
explain = select(explain, Not(Symbol(outcome_name)))  # Remove the outcome column.

reference = copy(boston)  # An optional reference population to compute the baseline prediction.
reference = select(reference, Not(Symbol(outcome_name)))

sample_size = 60  # Number of Monte Carlo samples.
#------------------------------------------------------------------------------
# Compute stochastic Shapley values.
data_shap = ShapML.shap(explain = explain,
                        reference = reference,
                        model = model,
                        predict_function = predict_function,
                        sample_size = sample_size,
                        parallel = :samples,  # Parallel computation over &quot;sample_size&quot;.
                        seed = 1
                        )

show(data_shap, allcols = true)</code></pre><p><img src="shapoutput.PNG" alt="shapoutput"/></p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="vignettes/consistency/">Stochastic vs. TreeSHAP »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 5 April 2020 02:18">Sunday 5 April 2020</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
